<meta name="description" content="Yawen Zeng&#39;s home page" charset="utf-8">
<link rel="stylesheet" href="file/index.css" type="text/css">
<link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.3.1/css/all.css" integrity="sha384-mzrmE5qonljUremFsqc01SB46JvROS7bZs3IO2EmfFsd15uHvIt+Y8vEf7N7fWAU" crossorigin="anonymous">
<script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>

<title>Yawen Zeng</title>
<body>
    <div id="layout-content" style="margin-top:25px">
        <table>
            <tbody>
                <tr>
                    <td width="670">
                        <div id="toptitle">
                            <h1>Yawen Zeng</h1>
                        </div>
                        <h3>An Advertisement: Seeking Collaborators or Interns</h3>
                        <p>
                            <br> <b style="unicode-bidi:bidi-override; direction: rtl;">moc.liamg@11gnezneway :liamE</span>
                            <br>
                        </p>
                    </td>
                    <td>
                        <img src="file/photo.jpeg" border="0" width="200">
                    </td>
                </tr>
                <tr>
                </tr>
            </tbody>
        </table>
        
        <h2>Biography</h2>
        <p>
            I am a Algorithm Engineer at ByteDance AI Lab. I obtained my M.S. from the College of Computer Science and Electronic Engineering at Hunan University in 2022. Prior to this, I gained valuable experience working with Tencent WeChat. My research interests are focused on multi-modal understanding, large vision language models, and information retrieval. I have had the privilege of publishing my work in esteemed academic conferences such as CVPR, AAAI, SIGIR, ACM MM, EMNLP, TNNLS, and TMM. Additionally, I am an active member of both IEEE and ACM.
        </p>
        <h2>News</h2>
        <ul>
            <li>(2023-08) Our work is accecpted by TMM 2023.</li>
            <li>(2023-04) Our work is accecpted by SIGIR 2023.</li>
            <li>(2023-04) I am awarded Excellent New Employee in ByteDance AI Lab.</li>
            <li>(2022-11) Our work is accecpted by AAAI 2023.</li>
            <li>(2022-10) Our work is accecpted by EMNLP 2022 as an Oral paper.</li>
            <li>(2022-04) Two papers are accecpted by ICMR 2022 as Oral papers.</li>
            <li>(2022-04) Our work is accecpted by TNNLS 2022.</li>
            <li>(2022-04) Our work is accecpted by SIGIR 2022.</li>
            <li>(2021-10) I am awarded National Scholarship for Postgraduate.</li>
            <li>(2021-07) Our work is accecpted by ACM MM 2021 as an Oral paper.</li>
            <li>(2021-03) Our work is accecpted by CVPR 2021.</li>
            <li>(2020-07) Two papers are accecpted by ACM MM 2020 as an Oral paper.</li>
        </ul>

        <h2>Selected Publications</h2>
        <tr>
            (*: equal contribution, â˜¨: correspondence)
        </tr>
        <table style="width:100%">
            <tr>
                <td></td>
            </tr>
        </table>
        
        <table style="width:100%">
            <tr>
                <td><a href="https://ieeexplore.ieee.org/abstract/document/10239444/">Temporally Language Grounding with Multi-modal Multi-Prompt Tuning</a></td>
            </tr>
            <tr>
                <td><b style="color:black">Yawen Zeng</b>, Ning Han, Keyu Pan, Qin Jin</td>
            </tr>
            <tr>
                <td>TMM, 2023</td>
            </tr>
        </table>

        <table style="width:100%">
            <tr>
                <td><a href="https://arxiv.org/abs/2307.16180">Do LLMs Possess a Personality? Making the MBTI Test an Amazing Evaluation for Large Language Models</a></td>
            </tr>
            <tr>
                <td>Keyu Pan, <b style="color:black">Yawen Zeng</b></td>
            </tr>
            <tr>
                <td>arXiv preprint, 2023</td>
            </tr>
        </table>

        <table style="width:100%">
            <tr>
                <td><a href="https://dl.acm.org/doi/abs/10.1145/3539618.3592054">RewardTLG: Learning to Temporally Language Grounding from Flexible Reward</a></td>
            </tr>
            <tr>
                <td><b style="color:black">Yawen Zeng</b>, Keyu Pan, Ning Han</td>
            </tr>
            <tr>
                <td>SIGIR, 2023</td>
            </tr>
        </table>

        <table style="width:100%">
            <tr>
                <td><a href="https://ojs.aaai.org/index.php/AAAI/article/view/25445">Multi-Modal Knowledge Hypergraph for Diverse Image Retrieval</a></td>
            </tr>
            <tr>
                <td><b style="color:black">Yawen Zeng</b>, Qin Jin, Tengfei Bao, Wenfeng Li</td>
            </tr>
            <tr>
                <td>AAAI, 2023</td>
            </tr>
        </table>

        <table style="width:100%">
            <tr>
                <td><a href="https://www.sciencedirect.com/science/article/abs/pii/S0957417423021036">Contrastive Topic-enhanced Network for Video Captioning</a></td>
            </tr>
            <tr>
                <td><b style="color:black">Yawen Zeng</b>, Yiru Wang, Dongliang Liao, Gongfu Li, Jin Xu, Bo Liu, Xiangmin Xu, Hong Man</td>
            </tr>
            <tr>
                <td>ESWA, 2023</td>
            </tr>
        </table>

        <table style="width:100%">
            <tr>
                <td><a href="https://dl.acm.org/doi/abs/10.1145/3627103">BiC-Net: Learning Efficient Spatio-Temporal Relation for Text-Video Retrieval</a></td>
            </tr>
            <tr>
                <td>Ning Han, <b style="color:black">Yawen Zeng</b>, Chuhao Shi, Guangyi Xiao, Hao Chen, Jingjing Chen</td>
            </tr>
            <tr>
                <td>TOMM, 2023</td>
            </tr>
        </table>

        <table style="width:100%">
            <tr>
                <td><a href="https://arxiv.org/abs/2304.10844">Better Sign Language Translation with Monolingual Data</a></td>
            </tr>
            <tr>
                <td>Ru Peng, <b style="color:black">Yawen Zeng</b>, Junbo Zhao</td>
            </tr>
            <tr>
                <td>arXiv preprint, 2023</td>
            </tr>
        </table>

        <table style="width:100%">
            <tr>
                <td><a href="https://ieeexplore.ieee.org/abstract/document/9764824/">Keyword-Based Diverse Image Retrieval with Variational Multiple Instance Graph</a></td>
            </tr>
            <tr>
                <td><b style="color:black">Yawen Zeng</b>, Yiru Wang, Dongliang Liao, Gongfu Li, Weijie Huang, Jin Xu, Da Cao, Hong Man</td>
            </tr>
            <tr>
                <td>TNNLS, 2022</td>
            </tr>
        </table>

        <table style="width:100%">
            <tr>
                <td><a href="https://dl.acm.org/doi/abs/10.1145/3477495.3531795">Point Prompt Tuning for Temporally Language Grounding</a></td>
            </tr>
            <tr>
                <td><b style="color:black">Yawen Zeng</b></td>
            </tr>
            <tr>
                <td>SIGIR, 2022</td>
            </tr>
        </table>

        <table style="width:100%">
            <tr>
                <td><a href="https://arxiv.org/abs/2210.04468">Distill the Image to Nowhere: Inversion Knowledge Distillation for Multimodal Machine Translation</a></td>
            </tr>
            <tr>
                <td>Ru Peng*, <b style="color:black">Yawen Zeng*</b>, Junbo Zhao</td>
            </tr>
            <tr>
                <td>EMNLP, 2022</td>
            </tr>
        </table>

        <table style="width:100%">
            <tr>
                <td><a href="https://dl.acm.org/doi/abs/10.1145/3512527.3531386">HybridVocab: Towards Multi-Modal Machine Translation via Multi-Aspect Alignment</a></td>
            </tr>
            <tr>
                <td>Ru Peng*, <b style="color:black">Yawen Zeng*</b>, Junbo Zhao</td>
            </tr>
            <tr>
                <td>ICMR, 2022</td>
            </tr>
        </table>

        <table style="width:100%">
            <tr>
                <td><a href="https://dl.acm.org/doi/abs/10.1145/3512527.3531397">TriReID: Towards Multi-Modal Person Re-Identification via Descriptive Fusion Model</a></td>
            </tr>
            <tr>
                <td>Yajing Zhai*, <b style="color:black">Yawen Zeng*</b>, Da Cao, Shaofei Lu</td>
            </tr>
            <tr>
                <td>ICMR, 2022</td>
            </tr>
        </table>

        <table style="width:100%">
            <tr>
                <td><a href="https://dl.acm.org/doi/abs/10.1145/3474085.3475241">Fine-grained cross-modal alignment network for text-video retrieval</a></td>
            </tr>
            <tr>
                <td>Ning Han, Jingjing Chen, Guangyi Xiao, Hao Zhang, <b style="color:black">Yawen Zeng</b>, Hao Chen</td>
            </tr>
            <tr>
                <td>ACM MM, 2021</td>
            </tr>
        </table>

        <table style="width:100%">
            <tr>
                <td><a href="https://dl.acm.org/doi/abs/10.1145/3478025">Moment is Important: Language-Based Video Moment Retrieval via Adversarial Learning</a></td>
            </tr>
            <tr>
                <td><b style="color:black">Yawen Zeng</b>, Da Cao, Hanling Zhang, Jiao Xu, Zheng Qin</td>
            </tr>
            <tr>
                <td>TOMM, 2021</td>
            </tr>
        </table>

        <table style="width:100%">
            <tr>
                <td><a href="https://openaccess.thecvf.com/content/CVPR2021/html/Zeng_Multi-Modal_Relational_Graph_for_Cross-Modal_Video_Moment_Retrieval_CVPR_2021_paper.html">Multi-Modal Relational Graph for Cross-Modal Video Moment Retrieval</a></td>
            </tr>
            <tr>
                <td><b style="color:black">Yawen Zeng</b>, Da Cao, Xiaochi Wei, Meng Liu, Zhou Zhao, Zheng Qin</td>
            </tr>
            <tr>
                <td>CVPR, 2021</td>
            </tr>
        </table>

        <table style="width:100%">
            <tr>
                <td><a href="https://dl.acm.org/doi/abs/10.1145/3394171.3413840">Adversarial Video Moment Retrieval by Jointly Modeling Ranking and Localization</a></td>
            </tr>
            <tr>
                <td>Da Cao, <b style="color:black">Yawen Zeng</b>, Xiaochi Wei, Liqiang Nie, Richang Hong, Zheng Qin</td>
            </tr>
            <tr>
                <td>ACM MM, 2020</td>
            </tr>
        </table>

        <table style="width:100%">
            <tr>
                <td><a href="https://dl.acm.org/doi/abs/10.1145/3394171.3413841">STRONG: Spatio-Temporal Reinforcement Learning for Cross-Modal Video Moment Localizationn</a></td>
            </tr>
            <tr>
                <td>Da Cao, <b style="color:black">Yawen Zeng</b>, Meng Liu, Xiangnan He, Meng Wang, Zheng Qin</td>
            </tr>
            <tr>
                <td>ACM MM, 2020</td>
            </tr>
        </table>

        <h2>Research Experience</h2>
        <table id="tbTeaching" width="100%">
            <tbody>
                <tr>
                    <td valign="top" width="16%">2022.07 - till now      </td>
                    <td><b class="Institute">BKB, ByteDance AI Lab</b><br>
                    <b class="Status">Algorithm Engineer</b><br>
                    <b class="Mentor">Supervisor: Hang Li, Tengfei Bao, Wenfeng Li</b><br>
                    <li class="Works" margin-left="10000px">Multi-Modal Knowledge Graph and Large Vision Language Models.</li>
                </tr>
                <tr>
                    <td> </td>
                    <td> </td>
                </tr>
                <tr>
                    <td> </td>
                    <td> </td>
                </tr>
                
                <tr>
                    <td valign="top" >2020.10 - 2022.06      </td>
                    <td><b class="Institute">WeChat, Tencent</b><br>
                    <b class="Status">Research Intern</b><br>
                    <b class="Mentor">Supervisor: Jin Xu, Gongfu Li</b><br>
                    <li class="Works" margin-left="10000px">Multi-Modal Retrieval and Visual Question Answering.</li>
                </tr>
                <tr>
                    <td> </td>
                    <td> </td>
                </tr>
                <tr>
                    <td> </td>
                    <td> </td>
                </tr>

                <tr>
                    <td valign="top" width="16%">2019.07 - 2022.06      </td>
                    <td><b class="Institute">College of Computer Science and Electronic Engineering, Hunan University</b><br>
                    <b class="Status">Master Student</b><br>
                    <b class="Mentor">Supervisor: Da Cao</b><br>
                    <li class="Works" margin-left="10000px">Temporally Language Grounding and Cross-Modal Retrieval.</li>
                </tr>
                <tr>
                    <td> </td>
                    <td> </td>
                </tr>
                <tr>
                    <td> </td>
                    <td> </td>
                </tr>
            </tbody>
        </table>


        <h2>Academic Service</h2>
            <table style="width:100%">
                <tr>
                    <td>Program Committee Member for EMNLP.</td>
                </tr>
            </table>
            <table style="width:100%">
                <tr>
                    <td>Conference Reviewer for CVPR, ICCV, ECCV, ACL, EMNLP, AAAI, ACM MM.</td>
                </tr>
            </table>
        </p>
        
        <div id="footer">
            <div id="footer-text"></div>
        </div>
    </div>

    <table style="width:100%">
        <tr>
            <td></td>
        </tr>
    </table>

    Last updated on August 2023. 

</body>

</html>