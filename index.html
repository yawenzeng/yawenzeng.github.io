<meta name="description" content="Yawen Zeng&#39;s home page" charset="utf-8">
<link rel="stylesheet" href="file/index.css" type="text/css">
<link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.3.1/css/all.css" integrity="sha384-mzrmE5qonljUremFsqc01SB46JvROS7bZs3IO2EmfFsd15uHvIt+Y8vEf7N7fWAU" crossorigin="anonymous">
<script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>

<title>Yawen Zeng</title>
<body>
    <div id="layout-content" style="margin-top:25px">
        <table>
            <tbody>
                <tr>
                    <td width="670">
                        <div id="toptitle">
                            <h1>Yawen Zeng</h1>
                        </div>
                        <p>
                            <br> <b style="unicode-bidi:bidi-override; direction: rtl;">moc.liamg@11gnezneway :liamE</span>
                            <br>
                        </p>
                    </td>
                    <td>
                        <img src="file/photo.jpeg" border="0" width="200">
                    </td>
                </tr>
                <tr>
                </tr>
            </tbody>
        </table>
        
        <h2>Biography</h2>
        <p>
            Hello, thanks for stopping by! I am a casual player focusing on Large Vision Language Models, General Agents, and Multi-Modal Applications. Due to my adventures in studying, interning, and working, I have collaborated with various institutions, including Hunan University, South China University of Technology, Tencent, and ByteDance, etc. I have had the privilege of publishing my research in esteemed academic conferences such as CVPR, AAAI, WWW, SIGIR, ICLR, TNNLS, and TMM.
        </p>
        <h2>News</h2>
        <ul>
            <li>(2025-01) Two papers are accecpted by WWW 2025 as Oral papers.</li>
            <li>(2025-01) One paper is accecpted by ICLR 2025.</li>
            <li>(2024-01) One paper is accecpted by WWW 2024.</li>
            <li>(2024-01) One paper is accecpted by ICLR 2024.</li>
            <li>(2023-12) Our work is accecpted by AAAI 2024.</li>
        </ul>
        
        <details>
            <summary>more news</summary>
                <ul>
                    <li>(2023-08) Our work is accecpted by TMM 2023.</li>
                    <li>(2023-04) Our work is accecpted by SIGIR 2023.</li>
                    <li>(2023-02) I am awarded Excellent New Employee in ByteDance AI Lab.</li>
                    <li>(2022-11) Our work is accecpted by AAAI 2023.</li>
                    <li>(2022-10) Our work is accecpted by EMNLP 2022 as an Oral paper.</li>
                    <li>(2022-04) Two papers are accecpted by ICMR 2022 as Oral papers.</li>
                    <li>(2022-04) Our work is accecpted by TNNLS 2022.</li>
                    <li>(2022-04) Our work is accecpted by SIGIR 2022.</li>
                    <li>(2021-07) One paper is accecpted by ACM MM 2021 as an Oral paper.</li>
                    <li>(2021-03) Our work is accecpted by CVPR 2021.</li>
                    <li>(2020-10) I am awarded National Scholarship for Postgraduate.</li>
                    <li>(2020-07) Two papers are accecpted by ACM MM 2020, one of which is an Oral paper.</li>
                    </ul>
        </details>

        <h2>Selected Publications</h2>
        <tr>
            (*: equal contribution, ☨: correspondence)
        </tr>
        <table style="width:100%">
            <tr>
                <td></td>
            </tr>
        </table>
        
        <table style="width:100%">
            <tr>
                <td><a href="https://arxiv.org/abs/2502.19363">DataMan: Data Manager for Pre-training Large Language Models</a></td>
            </tr>
            <tr>
                <td>Ru Peng, Kexin Yang, <b style="color:black">Yawen Zeng</b>, Junyang Lin, Dayiheng Liu, Junbo Zhao</td>
            </tr>
            <tr>
                <td>ICLR, 2025</td>
            </tr>
        </table>

        <table style="width:100%">
            <tr>
                <td><a href="https://arxiv.org/abs/2502.13165">HedgeAgents: A Balanced-aware Multi-agent Financial Trading System</a></td>
            </tr>
            <tr>
                <td>Xiangyu Li*, <b style="color:black">Yawen Zeng*</b>, Xiaofen Xing, Jin Xu and Xiangmin Xu</td>
            </tr>
            <tr>
                <td>WWW, 2025</td>
            </tr>
        </table>

        <table style="width:100%">
            <tr>
                <td><a href="https://arxiv.org/abs/2502.00792">RTBAgent: A LLM-based Agent System for Real-Time Bidding</a></td>
            </tr>
            <tr>
                <td>Leng Cai, Junxuan He, Yikai Li, Junjie Liang, Yuanping Lin, Ziming Quan, <b style="color:black">Yawen Zeng☨</b> and Jin Xu☨</td>
            </tr>
            <tr>
                <td>WWW, 2025</td>
            </tr>
        </table>

        <table style="width:100%">
            <tr>
                <td><a href="https://arxiv.org/abs/2401.12689">Energy-based Automated Model Evaluation</a></td>
            </tr>
            <tr>
                <td>Ru Peng, Heming Zou, Haobo Wang, <b style="color:black">Yawen Zeng</b>, Zenan Huang, Junbo Zhao</td>
            </tr>
            <tr>
                <td>ICLR, 2024</td>
            </tr>
        </table>
        
        <table style="width:100%">
            <tr>
                <td><a href="https://arxiv.org/abs/2403.02647">FinReport: Explainable Stock Earnings Forecasting via News Factor Analyzing Model</a></td>
            </tr>
            <tr>
                <td>Xiangyu Li, Xinjie Shen, <b style="color:black">Yawen Zeng</b>, Xiaofen Xing, Jin Xu</td>
            </tr>
            <tr>
                <td>WWW, 2024</td>
            </tr>
        </table>

        <table style="width:100%">
            <tr>
                <td><a href="https://arxiv.org/abs/2312.16797">Multi-Prompts Learning with Cross-Modal Alignment for Attribute-based Person Re-Identification</a></td>
            </tr>
            <tr>
                <td>Yajing Zhai*, <b style="color:black">Yawen Zeng*</b>, Zhiyong Huang, Zheng Qin, Xin Jin, Da Cao</td>
            </tr>
            <tr>
                <td>AAAI, 2024</td>
            </tr>
        </table>

        <table style="width:100%">
            <tr>
                <td><a href="https://arxiv.org/abs/2407.05355">VideoCoT: A Video Chain-of-Thought Dataset with Active Annotation Tool</a></td>
            </tr>
            <tr>
                <td>Yan Wang, <b style="color:black">Yawen Zeng☨</b>, Jingsheng Zheng, Xiaofen Xing, Jin Xu, Xiangmin Xu</td>
            </tr>
            <tr>
                <td>ACL, 2024 (workshop)</td>
            </tr>
        </table>

        <table style="width:100%">
            <tr>
                <td><a href="https://genai-personalization.github.io/assets/papers/GenAIRecP2024/Zeng.pdf">HindRec: Aligning User Preferences for Recommendation via Hindsight Fine-tuning</a></td>
            </tr>
            <tr>
                <td><b style="color:black">Yawen Zeng*</b>, Huanwen Wang*, Lingyu Chen, Wenshu Chen, Ran Chen, Hao Chen</td>
            </tr>
            <tr>
                <td>KDD, 2024 (workshop)</td>
            </tr>
        </table>

        <table style="width:100%">
            <tr>
                <td><a href="https://dl.acm.org/doi/10.1145/3652583.3658018">RetrievalMMT: Retrieval-Constrained Multi-Modal Prompt Learning for Multi-Modal Machine Translation</a></td>
            </tr>
            <tr>
                <td>Yan Wang*, <b style="color:black">Yawen Zeng*</b>, Junjie Liang, Xiaofen Xing, Jin Xu, Xiangmin Xu</td>
            </tr>
            <tr>
                <td>ICMR, 2024</td>
            </tr>
        </table>
        
        <table style="width:100%">
            <tr>
                <td><a href="https://ieeexplore.ieee.org/abstract/document/10239444/">Temporally Language Grounding with Multi-modal Multi-Prompt Tuning</a></td>
            </tr>
            <tr>
                <td><b style="color:black">Yawen Zeng</b>, Ning Han, Keyu Pan, Qin Jin</td>
            </tr>
            <tr>
                <td>TMM, 2023</td>
            </tr>
        </table>

        <table style="width:100%">
            <tr>
                <td><a href="https://arxiv.org/abs/2307.16180">Do LLMs Possess a Personality? Making the MBTI Test an Amazing Evaluation for Large Language Models</a></td>
            </tr>
            <tr>
                <td>Keyu Pan, <b style="color:black">Yawen Zeng</b></td>
            </tr>
            <tr>
                <td>arXiv preprint, 2023</td>
            </tr>
        </table>

        <table style="width:100%">
            <tr>
                <td><a href="https://dl.acm.org/doi/abs/10.1145/3539618.3592054">RewardTLG: Learning to Temporally Language Grounding from Flexible Reward</a></td>
            </tr>
            <tr>
                <td><b style="color:black">Yawen Zeng</b>, Keyu Pan, Ning Han</td>
            </tr>
            <tr>
                <td>SIGIR, 2023</td>
            </tr>
        </table>

        <table style="width:100%">
            <tr>
                <td><a href="https://ojs.aaai.org/index.php/AAAI/article/view/25445">Multi-Modal Knowledge Hypergraph for Diverse Image Retrieval</a></td>
            </tr>
            <tr>
                <td><b style="color:black">Yawen Zeng</b>, Qin Jin, Tengfei Bao, Wenfeng Li</td>
            </tr>
            <tr>
                <td>AAAI, 2023</td>
            </tr>
        </table>

        <table style="width:100%">
            <tr>
                <td><a href="https://www.sciencedirect.com/science/article/abs/pii/S0957417423021036">Contrastive Topic-enhanced Network for Video Captioning</a></td>
            </tr>
            <tr>
                <td><b style="color:black">Yawen Zeng</b>, Yiru Wang, Dongliang Liao, Gongfu Li, Jin Xu, Bo Liu, Xiangmin Xu, Hong Man</td>
            </tr>
            <tr>
                <td>ESWA, 2023</td>
            </tr>
        </table>

        <table style="width:100%">
            <tr>
                <td><a href="https://dl.acm.org/doi/abs/10.1145/3627103">BiC-Net: Learning Efficient Spatio-Temporal Relation for Text-Video Retrieval</a></td>
            </tr>
            <tr>
                <td>Ning Han, <b style="color:black">Yawen Zeng</b>, Chuhao Shi, Guangyi Xiao, Hao Chen, Jingjing Chen</td>
            </tr>
            <tr>
                <td>TOMM, 2023</td>
            </tr>
        </table>

        <table style="width:100%">
            <tr>
                <td><a href="https://arxiv.org/abs/2304.10844">Better Sign Language Translation with Monolingual Data</a></td>
            </tr>
            <tr>
                <td>Ru Peng, <b style="color:black">Yawen Zeng</b>, Junbo Zhao</td>
            </tr>
            <tr>
                <td>arXiv preprint, 2023</td>
            </tr>
        </table>

        <table style="width:100%">
            <tr>
                <td><a href="https://ieeexplore.ieee.org/abstract/document/9764824/">Keyword-Based Diverse Image Retrieval with Variational Multiple Instance Graph</a></td>
            </tr>
            <tr>
                <td><b style="color:black">Yawen Zeng</b>, Yiru Wang, Dongliang Liao, Gongfu Li, Weijie Huang, Jin Xu, Da Cao, Hong Man</td>
            </tr>
            <tr>
                <td>TNNLS, 2022</td>
            </tr>
        </table>

        <table style="width:100%">
            <tr>
                <td><a href="https://dl.acm.org/doi/abs/10.1145/3477495.3531795">Point Prompt Tuning for Temporally Language Grounding</a></td>
            </tr>
            <tr>
                <td><b style="color:black">Yawen Zeng</b></td>
            </tr>
            <tr>
                <td>SIGIR, 2022</td>
            </tr>
        </table>

        <table style="width:100%">
            <tr>
                <td><a href="https://arxiv.org/abs/2210.04468">Distill the Image to Nowhere: Inversion Knowledge Distillation for Multimodal Machine Translation</a></td>
            </tr>
            <tr>
                <td>Ru Peng*, <b style="color:black">Yawen Zeng*</b>, Junbo Zhao</td>
            </tr>
            <tr>
                <td>EMNLP, 2022</td>
            </tr>
        </table>

        <table style="width:100%">
            <tr>
                <td><a href="https://dl.acm.org/doi/abs/10.1145/3512527.3531386">HybridVocab: Towards Multi-Modal Machine Translation via Multi-Aspect Alignment</a></td>
            </tr>
            <tr>
                <td>Ru Peng*, <b style="color:black">Yawen Zeng*</b>, Junbo Zhao</td>
            </tr>
            <tr>
                <td>ICMR, 2022</td>
            </tr>
        </table>

        <table style="width:100%">
            <tr>
                <td><a href="https://dl.acm.org/doi/abs/10.1145/3512527.3531397">TriReID: Towards Multi-Modal Person Re-Identification via Descriptive Fusion Model</a></td>
            </tr>
            <tr>
                <td>Yajing Zhai*, <b style="color:black">Yawen Zeng*</b>, Da Cao, Shaofei Lu</td>
            </tr>
            <tr>
                <td>ICMR, 2022</td>
            </tr>
        </table>

        <table style="width:100%">
            <tr>
                <td><a href="https://dl.acm.org/doi/abs/10.1145/3474085.3475241">Fine-grained cross-modal alignment network for text-video retrieval</a></td>
            </tr>
            <tr>
                <td>Ning Han, Jingjing Chen, Guangyi Xiao, Hao Zhang, <b style="color:black">Yawen Zeng</b>, Hao Chen</td>
            </tr>
            <tr>
                <td>ACM MM, 2021</td>
            </tr>
        </table>

        <table style="width:100%">
            <tr>
                <td><a href="https://dl.acm.org/doi/abs/10.1145/3478025">Moment is Important: Language-Based Video Moment Retrieval via Adversarial Learning</a></td>
            </tr>
            <tr>
                <td><b style="color:black">Yawen Zeng</b>, Da Cao, Hanling Zhang, Jiao Xu, Zheng Qin</td>
            </tr>
            <tr>
                <td>TOMM, 2021</td>
            </tr>
        </table>

        <table style="width:100%">
            <tr>
                <td><a href="https://openaccess.thecvf.com/content/CVPR2021/html/Zeng_Multi-Modal_Relational_Graph_for_Cross-Modal_Video_Moment_Retrieval_CVPR_2021_paper.html">Multi-Modal Relational Graph for Cross-Modal Video Moment Retrieval</a></td>
            </tr>
            <tr>
                <td><b style="color:black">Yawen Zeng</b>, Da Cao, Xiaochi Wei, Meng Liu, Zhou Zhao, Zheng Qin</td>
            </tr>
            <tr>
                <td>CVPR, 2021</td>
            </tr>
        </table>

        <table style="width:100%">
            <tr>
                <td><a href="https://dl.acm.org/doi/abs/10.1145/3394171.3413840">Adversarial Video Moment Retrieval by Jointly Modeling Ranking and Localization</a></td>
            </tr>
            <tr>
                <td>Da Cao, <b style="color:black">Yawen Zeng</b>, Xiaochi Wei, Liqiang Nie, Richang Hong, Zheng Qin</td>
            </tr>
            <tr>
                <td>ACM MM, 2020</td>
            </tr>
        </table>

        <table style="width:100%">
            <tr>
                <td><a href="https://dl.acm.org/doi/abs/10.1145/3394171.3413841">STRONG: Spatio-Temporal Reinforcement Learning for Cross-Modal Video Moment Localizationn</a></td>
            </tr>
            <tr>
                <td>Da Cao, <b style="color:black">Yawen Zeng</b>, Meng Liu, Xiangnan He, Meng Wang, Zheng Qin</td>
            </tr>
            <tr>
                <td>ACM MM, 2020</td>
            </tr>
        </table>

        <h2>Academic Service</h2>
            <table style="width:100%">
                <tr>
                    <td>Conference Reviewer for CVPR, ICCV, ECCV, ACL, EMNLP, KDD, WWW, NeurIPS, AAAI.</td>
                </tr>
            </table>
            <table style="width:100%">
                <tr>
                    <td>Journal Reviewer for TPAMI, TNNLS, TMM, TKDE, TKDD, TOMM.</td>
                </tr>
            </table>
        </p>
        
        <div id="footer">
            <div id="footer-text"></div>
        </div>
    </div>

    <table style="width:100%">
        <tr>
            <td></td>
        </tr>
    </table>

    Last updated on Jan 2025. 

</body>

</html>
